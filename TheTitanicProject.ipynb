{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#The Titanic Project"
      ],
      "metadata": {
        "id": "NyYpyksqv2Kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy and paste/type thesse codes in your Google Colab. Give your inputs in the specific checkpoints"
      ],
      "metadata": {
        "id": "ffdMKW3D2IDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this to remove the unnecessary warnings."
      ],
      "metadata": {
        "id": "8vt9MnsvwCTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "NdKm4dlWb7Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import the necessary libraries"
      ],
      "metadata": {
        "id": "I6XM5P-JwJ_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown"
      ],
      "metadata": {
        "id": "MEviniT3dQ2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mount google drive in the colab environment. Make sure the account should have the csv file in it. You can locate it in:\n",
        "drive->MyDrive->locate your file->right click->copy path"
      ],
      "metadata": {
        "id": "JQRE3PhGwOsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "jTGypkBjartp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "paste your file path within the ( ) (Use the quotation marks!)"
      ],
      "metadata": {
        "id": "ba6Iq0PfwU5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= pd.read_csv(\"/content/drive/MyDrive/train (1).csv\")"
      ],
      "metadata": {
        "id": "sNycHG62Zxl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "displays the first few rows of the dataset"
      ],
      "metadata": {
        "id": "1DIxC7RuwfUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "Ox8VSkaCasAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKPOINT 1 -- Write the code to display the \"last\" few rows of the dataset\n"
      ],
      "metadata": {
        "id": "dHH45_KId9-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we remove unneccessary features"
      ],
      "metadata": {
        "id": "Ilz-udRFwmO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature = [\"Name\",\"Ticket\",\"Cabin\"]\n",
        "data.drop(feature, axis = 1, inplace= True)"
      ],
      "metadata": {
        "id": "kJ02FRKRbL-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will be vizualizing the data"
      ],
      "metadata": {
        "id": "daDfdxUOwsva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = sns.catplot(x=\"Sex\", y=\"Survived\", data=data, kind=\"bar\", height=4, hue=\"Pclass\")\n",
        "x.set_ylabels(\"Survived Probability\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W7Gj5eXIbkKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the density distribution"
      ],
      "metadata": {
        "id": "2HaXBq-fwv1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.kdeplot(data=data, x='Survived', fill=True, hue = \"Pclass\")\n",
        "plt.title('Density Distribution of Survived Passengers')\n",
        "plt.xlabel('Survived')\n",
        "plt.ylabel('Age')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zOuze-fXbuWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot a stacked bar chart for Died and Survived."
      ],
      "metadata": {
        "id": "rqaQA8_Gw2a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Died\"]=  1 - data[\"Survived\"]\n",
        "data.groupby(\"Embarked\").agg(\"sum\")[[\"Survived\", \"Died\"]].plot(kind= \"bar\", figsize=(6,4), stacked = True)"
      ],
      "metadata": {
        "id": "AppK-HLKeaaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKPOINT 2-- Try to draw a boxplot using any two columns. Dont use the columns which have been removed!\n"
      ],
      "metadata": {
        "id": "qHhquL4Qe9i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There might be NaN/null values in a dataset which can cause problems. We need to remove it.\n",
        "lets use the \"mean\" values to replace these values.\n",
        "You can also experiment by using the \"Median\" value to replace, by simply replacing the function"
      ],
      "metadata": {
        "id": "qX_qJI3Sw5kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_age= data[\"Age\"].mean()\n",
        "data[\"Age\"]= data[\"Age\"].fillna(mean_age)"
      ],
      "metadata": {
        "id": "bJO91cdyfpXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "yT2w57JOg5Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that Embarked has 2 null values, but this column has text values, so we can find Mean/median.\n",
        "We simply drop these values"
      ],
      "metadata": {
        "id": "XlebJWQCxA-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(subset=[\"Embarked\"], inplace=True)"
      ],
      "metadata": {
        "id": "Gtc6v27ThRU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "_31YOLMEhiUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our Machine Learning Algorithm to work, we need all the value to be numerical. So do we have any textual data?"
      ],
      "metadata": {
        "id": "zqXYrFFExKzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "EV4idZvnh5pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, \"Sex\" and \"Embarked\" are non-integer/non-float...so we need to convert it!\n",
        "We will do Label encoding to these columns which will encode the text values in numerical form.\n",
        "For Example, Sex \"Male\" gets encoded as \"1\""
      ],
      "metadata": {
        "id": "vhw3F1e_xRDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "data[\"Sex\"] = le.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = le.fit_transform(data[\"Embarked\"])"
      ],
      "metadata": {
        "id": "-RBBRz76i6-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "NzDdxDUNjupk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All data are now numerical. But wait! The Age and Fare are not lying in the same range as of other columns.\n",
        "This causes large deviations in values, causing smaller values getting neglected.\n",
        "To address this we use Scaling. We will use Standard Scaler"
      ],
      "metadata": {
        "id": "JZ5ji5ihxdTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "data[['Age', 'Fare']] = scaler.fit_transform(data[['Age', 'Fare']])"
      ],
      "metadata": {
        "id": "Hdmc1KF-jy0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For machine learning algorithms, we need independent and dependent features.\n",
        "independent features are the parameters and dependent features are the values derived from this parameter\n",
        "machine learning models will use these two to find relations and give predictions on newer set of independent features."
      ],
      "metadata": {
        "id": "9MeKhCk7xhPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.drop([\"Survived\", \"Died\"], axis= 1) #independent features\n",
        "y = data[[\"Survived\"]] #dependent features"
      ],
      "metadata": {
        "id": "vCkGckeZkYMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we split the data into training set and testing set.\n",
        "In machine learning, the training set is used to train the model and learn patterns,\n",
        "while the testing set is used to evaluate the modelâ€™s performance on unseen data123.\n",
        "This split is crucial to assess how well the trained model generalizes to new data\n",
        "test size is generally kept 0.2, so it divides data into 0.8 (80%) train and 0.2 (20%) test. Its generally recommended\n",
        "test size should be kept small to avoid model overfitting\n",
        "The random_state parameter in machine learning is used to control the randomness\n",
        "for data shuffling and other random procedures. It ensures that the random processes\n",
        "produce the same results each time you run your code,\n",
        "which is important for reproducibility\n"
      ],
      "metadata": {
        "id": "i1bgR-Pmxkea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xtrain,xtest,ytrain,ytest= train_test_split(x,y, test_size=.20, random_state=42)\n"
      ],
      "metadata": {
        "id": "MXRPApXrkZ_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"xtrain shape:\", xtrain.shape)\n",
        "print(\"xtest shape:\", xtest.shape)\n",
        "print(\"ytrain shape:\", ytrain.shape)\n",
        "print(\"ytest shape:\", ytest.shape)"
      ],
      "metadata": {
        "id": "5L2PL3yEnEgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model import time! Lets first use the most common classification models."
      ],
      "metadata": {
        "id": "l5RahiexxpZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "XLDeb6ysnOZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors= 5) #n_neighbours = number of neighbors\n",
        "svmc = SVC(C = 5) #C -> regularization parameter for better tradeoff between low training and low testing error"
      ],
      "metadata": {
        "id": "JaH19nCSoNxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit the training data in the models"
      ],
      "metadata": {
        "id": "kZFlJtQtxvRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn.fit(xtrain, ytrain)"
      ],
      "metadata": {
        "id": "yaumIdc6o7TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKPOINT 3 --- fit the training data in the SVM model\n"
      ],
      "metadata": {
        "id": "XJ7kCQrHpFxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets make the helper function to analyse how well our model is performing on training and testing data\n",
        "xtrain xtest ytest are global variables so we dont need to declare again."
      ],
      "metadata": {
        "id": "b22WtcRGxxXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "def acc(model):\n",
        "    model.fit(xtrain, ytrain)\n",
        "    train_predictions = model.predict(xtrain)\n",
        "    test_predictions = model.predict(xtest)\n",
        "    train_accuracy = accuracy_score(ytrain, train_predictions)\n",
        "    test_accuracy = accuracy_score(ytest, test_predictions)\n",
        "    print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
        "    print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "S40IxrX6pYy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets now use it. You can use it for the svm to check the results as well.\n",
        "acc(knn)"
      ],
      "metadata": {
        "id": "JAEW-qVRp8WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we observe the training accuracy is kinda good. But we need better testing accuracy since it tells how model is performing\n",
        "on unseen data.\n",
        "Lets call the heavyweights, ensemble algorithms!"
      ],
      "metadata": {
        "id": "1kRL0dKwx7rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "wZw0Y-tXquSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnd= RandomForestClassifier(n_estimators= 800,ccp_alpha=.01) #n_estimators decide the number of trees\n",
        "#ccp_alpha is a complexity parameter which uses Minimal Cost Complexity Pruning\n",
        "#Here higher values gets pruned, controlling tree size\n",
        "grb = GradientBoostingClassifier(learning_rate = 0.001, n_estimators=1000)\n",
        "#learning rate basically controls how quickly or slowly model learns from trainig data"
      ],
      "metadata": {
        "id": "bbpO9F73rd7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnd.fit(xtrain,ytrain)\n",
        "grb.fit(xtrain,ytrain)"
      ],
      "metadata": {
        "id": "jm0f_rDJtfqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc(rnd)"
      ],
      "metadata": {
        "id": "fW08NnuwtmYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc(grb)"
      ],
      "metadata": {
        "id": "uWC0d3hZtyV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thats some great improvement! Clearly shows the power of ensemble algorithms."
      ],
      "metadata": {
        "id": "FZrZBpoPyEF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKPOINT 4-- Now import, initiate and fit the training data for XGBoost Model. Use the acc() helper function.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j7Acp-1FuNRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we want our model to predict on real time values. Here is the function:\n",
        "def predict_survival(model):\n",
        "    PassengerId = int(input(\"Enter PassengerId: \"))\n",
        "    Pclass = int(input(\"Enter Pclass: \"))\n",
        "    Sex = input(\"Enter Sex: \")\n",
        "    Age = float(input(\"Enter Age: \"))\n",
        "    SibSp = int(input(\"Enter SibSp: \"))\n",
        "    Parch = int(input(\"Enter Parch: \"))\n",
        "    Fare = float(input(\"Enter Fare: \"))\n",
        "    Embarked = input(\"Enter Embarked: \")\n",
        "    new_data = pd.DataFrame({\n",
        "        'PassengerId': [PassengerId],\n",
        "        'Pclass': [Pclass],\n",
        "        'Sex': [Sex],\n",
        "        'Age': [Age],\n",
        "        'SibSp': [SibSp],\n",
        "        'Parch': [Parch],\n",
        "        'Fare': [Fare],\n",
        "        'Embarked': [Embarked]\n",
        "    })\n",
        "    prediction = model.predict(new_data)\n",
        "    return \"Passenger Survived\" if prediction[0] == 1 else \"Passesnger Did not survive\""
      ],
      "metadata": {
        "id": "z3fbFMIquagv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_survival(grb) #Try with different models."
      ],
      "metadata": {
        "id": "W1xZS0CRvFmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All set! You have successfully completed your first ever Machine learning project!!\n",
        "#Optionally you can try to acquire all the testing accuracies of different models and maybe try to plot a graph of those"
      ],
      "metadata": {
        "id": "sJAgGHj0vM4g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}