{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##How do they Visualize?\n",
        "A basic project vizualizing the learning process of Neural Networks"
      ],
      "metadata": {
        "id": "rrE5Kdq88j_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving forward make sure you activate the Hardware accelerator (GPU) on this notebook. For the basics, you get the T4 GPU for free. Use it"
      ],
      "metadata": {
        "id": "-Yusbwcw85hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Update certain Dependencies"
      ],
      "metadata": {
        "id": "y_ccUvPS82EQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSbtxJox4FO8"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tf-keras-vis tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tf_keras_vis.utils import num_of_gpus\n",
        "\n",
        "_, gpus = num_of_gpus()\n",
        "print('{} GPUs'.format(gpus)) #check if gpu available. If its zero, use hardware accelerator.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2xbbqhK4XGJ",
        "outputId": "1fa694cc-a1e2-4a9e-bacb-f92a4681c777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step2: Import the VGG 16 model for testing."
      ],
      "metadata": {
        "id": "tjbd6jWn9KaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16 as Model\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# Load model\n",
        "model = Model(weights='imagenet', include_top=True)\n",
        "#model.summary()  #Uncomment this part to see the underlying structure of the model."
      ],
      "metadata": {
        "id": "8DjndQHM4zUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step3: Download the test images"
      ],
      "metadata": {
        "id": "NVh6THHR9Znz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the example datasets\n",
        "!wget https://github.com/keisen/tf-keras-vis/raw/master/docs/examples/images/goldfish.jpg\n",
        "!wget https://github.com/keisen/tf-keras-vis/raw/master/docs/examples/images/bear.jpg\n",
        "!wget https://github.com/keisen/tf-keras-vis/raw/master/docs/examples/images/soldiers.jpg"
      ],
      "metadata": {
        "id": "867WK1p35KGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step4: Load and Display the text images"
      ],
      "metadata": {
        "id": "EvLd_Cyc9l4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# Image titles\n",
        "image_titles = ['Goldfish', 'Bear', 'Assault rifle']\n",
        "\n",
        "# Load images\n",
        "img1 = load_img('goldfish.jpg', target_size=(224, 224))\n",
        "img2 = load_img('bear.jpg', target_size=(224, 224))\n",
        "img3 = load_img('soldiers.jpg', target_size=(224, 224))\n",
        "images = np.asarray([np.array(img1), np.array(img2), np.array(img3)])\n",
        "\n",
        "# Preparing input data\n",
        "X = preprocess_input(images)\n",
        "\n",
        "# Code to display the image\n",
        "subplot_args = { 'nrows': 1, 'ncols': 3, 'figsize': (9, 3),\n",
        "                 'subplot_kw': {'xticks': [], 'yticks': []} }\n",
        "f, ax = plt.subplots(**subplot_args)\n",
        "for i, title in enumerate(image_titles):\n",
        "    ax[i].set_title(title, fontsize=14)\n",
        "    ax[i].imshow(images[i])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2nn7l2cS5PXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step5: Load the model outputs and modify it to get the raw outputs for the visuzalization."
      ],
      "metadata": {
        "id": "-hfD1jf7BEBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(output):\n",
        "    # 1 is the imagenet index corresponding to Goldfish, 294 to Bear and 413 to Assault Rifle.\n",
        "    return (output[0][1], output[1][294], output[2][413])\n",
        "def model_modifier(m):\n",
        "    m.layers[-1].activation = tf.keras.activations.linear\n",
        "    return m"
      ],
      "metadata": {
        "id": "Q4qPAVUG5h9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step6: We run different vizualization models and see the output"
      ],
      "metadata": {
        "id": "yoFO_jSUZI5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grad-CAM, or Gradient-weighted Class Activation Mapping, is a visualization technique for deep learning networks that helps in understanding which parts of a given image led a Convolutional Neural Network (CNN) to its final decision. It uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. This technique is widely used for making CNN-based models more transparent and explainable."
      ],
      "metadata": {
        "id": "X1Wf1hS7BdAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tensorflow.keras import backend as K\n",
        "from tf_keras_vis.utils import normalize\n",
        "from matplotlib import cm\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "\n",
        "# Create Gradcam object\n",
        "gradcam = Gradcam(model,\n",
        "                  model_modifier=model_modifier,\n",
        "                  clone=False)\n",
        "\n",
        "# Generate heatmap with GradCAM\n",
        "cam = gradcam(loss,\n",
        "              X,\n",
        "              penultimate_layer=-1, # model.layers number\n",
        "             )\n",
        "cam = normalize(cam)\n",
        "\n",
        "#below code is to display the heatmaps for the gradcam\n",
        "\n",
        "f, ax = plt.subplots(**subplot_args)\n",
        "for i, title in enumerate(image_titles):\n",
        "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "    ax[i].set_title(title, fontsize=14)\n",
        "    ax[i].imshow(images[i])\n",
        "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5) # overlay\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fx9F6PSP5t6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grad-CAM++ is an advanced version of Grad-CAM, offering improvements in the visualization of convolutional neural networks (CNNs). It extends the Grad-CAM method by using a weighted combination of the positive partial derivatives of the last convolutional layer’s feature maps with respect to a specific class score. This results in more accurate and detailed visual explanations, highlighting multiple relevant regions in the image instead of just one. Grad-CAM++ is particularly useful for identifying fine-grained features that contribute to a CNN’s decision, making it a valuable tool for interpreting complex models."
      ],
      "metadata": {
        "id": "kQJnFDKgbGmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKPOINT 1 : Implement the Gradcampluplus function, following through the same code as above and replace the gradcam function with gradcamplusplus and check the results"
      ],
      "metadata": {
        "id": "zQjf9tlVbGKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score-CAM, or Score-weighted Class Activation Mapping, is an interpretability method for Convolutional Neural Networks (CNNs) that builds on the idea of Grad-CAM. Unlike Grad-CAM, which uses gradient information to weigh the activation maps, Score-CAM uses the scores of the target class before the softmax layer to create a weighted combination of activation maps. This approach avoids potential issues with using gradients and provides a more straightforward and effective visualization of the areas in an image that contribute to a model’s decision."
      ],
      "metadata": {
        "id": "a2Tm1s4YaCa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from tf_keras_vis.scorecam import ScoreCAM\n",
        "\n",
        "# Create ScoreCAM object\n",
        "scorecam = ScoreCAM(model, model_modifier, clone=False)\n",
        "if gpus > 0:\n",
        "    # Generate heatmap with ScoreCAM\n",
        "    cam = scorecam(loss,\n",
        "                   X,\n",
        "                   penultimate_layer=-1, # model.layers number\n",
        "                  )\n",
        "    cam = normalize(cam)\n",
        "\n",
        "    f, ax = plt.subplots(**subplot_args)\n",
        "    for i, title in enumerate(image_titles):\n",
        "        heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "        ax[i].set_title(title, fontsize=14)\n",
        "        ax[i].imshow(images[i])\n",
        "        ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"NOTE: Change to GPU to see visual output\\n\")"
      ],
      "metadata": {
        "id": "2U3mOVR656bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faster Score-CAM appears to be a variant of Score-CAM that is designed to generate visual explanations more quickly. While the standard Score-CAM method avoids the use of gradients and instead uses forward passing scores to weigh activation maps for visual explanations, Faster Score-CAM likely incorporates optimizations that speed up this process. This could involve more efficient computation or approximation techniques to reduce the time required to visualize Class Activation Maps (CAMs) while still highlighting the influential regions in an image for a CNN’s decision-making process"
      ],
      "metadata": {
        "id": "C8qcxt3QbBd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Create ScoreCAM object\n",
        "scorecam = ScoreCAM(model, model_modifier, clone=False)\n",
        "\n",
        "# Generate heatmap with Faster-ScoreCAM\n",
        "cam = scorecam(loss,\n",
        "               X,\n",
        "               penultimate_layer=-1, # model.layers number\n",
        "               max_N=10\n",
        "              )\n",
        "cam = normalize(cam)\n",
        "\n",
        "f, ax = plt.subplots(**subplot_args)\n",
        "for i, title in enumerate(image_titles):\n",
        "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "    ax[i].set_title(title, fontsize=14)\n",
        "    ax[i].imshow(images[i])\n",
        "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "prQc0sO-6Lhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nEhVNNCS7HbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}